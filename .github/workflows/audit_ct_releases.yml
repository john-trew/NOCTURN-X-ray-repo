name: Audit CT Text Releases

on:
  workflow_dispatch:
    inputs:
      days_to_check:
        description: 'Number of days to look back (max 100)'
        required: true
        default: '30'
        type: string

jobs:
  audit_releases:
    runs-on: ubuntu-latest
    steps:
      - name: Check out repo
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.9"

      - name: Install Dependencies
        run: pip install pandas requests

      - name: Create Python Script
        run: |
          cat > analyze_releases.py << 'EOF'
          import json,sys,os,requests,pandas as pd
          from datetime import datetime,timedelta
          token=os.environ["GITHUB_TOKEN"]
          headers={"Authorization":f"Bearer {token}"}
          days=int(os.environ["DAYS"])
          end_date=datetime.now()
          start_date=end_date-timedelta(days=days)
          releases=[]
          page=1
          while True:
              r=requests.get(f"https://api.github.com/repos/{os.environ['GITHUB_REPOSITORY']}/releases?per_page=100&page={page}",headers=headers)
              if not r.ok or not r.json():break
              batch=[]
              for rel in r.json():
                  if not rel["tag_name"].startswith("ct_to_text_analysis-"):continue
                  created=datetime.strptime(rel["created_at"],"%Y-%m-%dT%H:%M:%SZ")
                  if created < start_date:break
                  morpho_ref=next((line for line in rel["body"].split("\n") if "morphosource-updates-" in line),"No reference found")
                  commit_hash=next((line.split()[0] for line in rel["body"].split("\n") if len(line.split()) > 0 and all(c in "0123456789abcdef" for c in line.split()[0]) and len(line.split()[0])==7),"unknown")
                  # Escape newlines in body for CSV
                  body_escaped = rel["body"].replace("\n", "\\n") if rel["body"] else ""
                  batch.append({
                      "ct_release": rel["tag_name"],
                      "created_at": rel["created_at"],
                      "morphosource_ref": morpho_ref,
                      "commit": commit_hash,
                      "release_body": body_escaped
                  })
              if not batch:break
              releases.extend(batch)
              page+=1
          
          df=pd.DataFrame(releases)
          df["date"]=pd.to_datetime(df["created_at"])
          
          # Save all releases in loops to detailed CSV
          loop_releases = df[df.groupby("commit")["commit"].transform("count") > 1].sort_values(["commit", "created_at"])
          if not loop_releases.empty:
              loop_releases.to_csv("all_duplicate_releases.csv", index=False, quoting=1)  # Force quotes around all fields
          
          # Create summary data
          commit_groups = df.groupby("commit")
          loop_data = []
          
          for commit, group in commit_groups:
              if len(group) > 1:
                  first_release = group.iloc[0]
                  loop_data.append({
                      "commit": commit,
                      "first_release": first_release["ct_release"],
                      "first_time": first_release["created_at"],
                      "morphosource_ref": first_release["morphosource_ref"],
                      "repeat_count": len(group),
                      "time_span_hours": (group["date"].max() - group["date"].min()).total_seconds() / 3600
                  })
          
          if loop_data:
              loops_df = pd.DataFrame(loop_data)
              loops_df.to_csv("duplicate_releases_summary.csv", index=False)
              print("\nFound workflow loops:")
              print("\nCommits that triggered multiple releases:")
              for _, row in loops_df.iterrows():
                  print(f"\nCommit {row['commit']}:")
                  print(f"  Morphosource ref: {row['morphosource_ref']}")
                  print(f"  First release: {row['first_release']}")
                  print(f"  Number of repeats: {row['repeat_count']}")
                  print(f"  Loop duration: {row['time_span_hours']:.1f} hours")
          else:
              print("\nNo workflow loops found")
          EOF

      - name: Run Analysis
        id: analyze
        env:
          GITHUB_TOKEN: ${{ secrets.MY_GITHUB_TOKEN }}
          DAYS: ${{ inputs.days_to_check }}
        run: |
          python3 analyze_releases.py
          if [ -f "duplicate_releases_summary.csv" ]; then
            echo "has_duplicates=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_duplicates=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Upload Results
        if: steps.analyze.outputs.has_duplicates == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: duplicate-releases-report
          path: |
            duplicate_releases_summary.csv
            all_duplicate_releases.csv
          retention-days: 90

      - name: Display Summary
        if: steps.analyze.outputs.has_duplicates == 'true'
        run: |
          echo "=== Duplicate Releases Report ==="
          echo "Full details available in the artifacts section"
          echo ""
          echo "Preview of duplicates (showing time differences):"
          head -n 5 duplicate_releases_summary.csv 